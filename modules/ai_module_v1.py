"""
AI Module - Reinforcement Learning Policy

Chloe Lee (cl4490) ë‹´ë‹¹ ëª¨ë“ˆ
PPO/DQN ê¸°ë°˜ ê²Œì„ AI ì •ì±…

ë‚œì´ë„ ë ˆë²¨ ì‹œìŠ¤í…œ:
- Level 1 (Easy): ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± (ê¸°ë³¸ íšŒí”¼ë§Œ)
- Level 2 (Medium): ê³ ê¸‰ íœ´ë¦¬ìŠ¤í‹± (íšŒí”¼ + ë³„ ìˆ˜ì§‘ ì „ëµ)
- Level 3 (Hard): PPO ëª¨ë¸ ê¸°ë°˜ (ì—†ìœ¼ë©´ ìµœê³ ê¸‰ íœ´ë¦¬ìŠ¤í‹±)

TODO for Chloe:
1. simulate_ai_decision() â†’ real_ppo_decision() êµì²´
2. ì •ì±… ë„¤íŠ¸ì›Œí¬ í›ˆë ¨ ë° ë¡œë“œ
3. ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ìµœì í™”
4. ìê°€ í•™ìŠµ (Self-Play) êµ¬í˜„
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import time
import random
from pathlib import Path

# PyTorchëŠ” ì„ íƒì  (ì‹¤ì œ RL ëª¨ë¸ êµ¬í˜„ ì‹œ í•„ìš”)
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch (torch) ì—†ìŒ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë§Œ ì‚¬ìš© ê°€ëŠ¥")
    # ë”ë¯¸ í´ë˜ìŠ¤ (íƒ€ì… íŒíŠ¸ìš©)
    class nn:
        class Module:
            pass
        class Sequential:
            pass
        class Linear:
            pass
        class ReLU:
            pass
        class Softmax:
            pass

# TODO: Chloeê°€ ì¶”ê°€í•  import
# from stable_baselines3 import PPO, DQN
# from ..src.utils.rl_instrumentation import RLInstrumentationLogger


class PolicyNetwork(nn.Module):
    """
    ì •ì±… ë„¤íŠ¸ì›Œí¬ (MLP)
    
    Chloeê°€ êµ¬í˜„í•  ì‹ ê²½ë§ êµ¬ì¡°
    """
    
    def __init__(self, state_dim: int = 8, hidden_dim: int = 128, action_dim: int = 4):
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch (torch)ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤ì œ RL ëª¨ë¸ êµ¬í˜„ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.network(state)


class ValueNetwork(nn.Module):
    """
    ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ (PPOìš©)
    
    Chloeê°€ PPO êµ¬í˜„ ì‹œ ì‚¬ìš©
    """
    
    def __init__(self, state_dim: int = 8, hidden_dim: int = 128):
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch (torch)ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤ì œ RL ëª¨ë¸ êµ¬í˜„ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.network(state)


class AIDecisionResult:
    """AI ì˜ì‚¬ê²°ì • ê²°ê³¼"""
    
    def __init__(self, action: str, confidence: float, reasoning: str = "", 
                 action_probs: Optional[Dict[str, float]] = None):
        self.action = action
        self.confidence = confidence
        self.reasoning = reasoning
        self.action_probs = action_probs or {}
        self.timestamp = time.time()
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì›¹ ì „ì†¡ìš©)"""
        return {
            'action': self.action,
            'confidence': self.confidence,
            'reasoning': self.reasoning,
            'action_probs': self.action_probs,
            'timestamp': self.timestamp
        }


class AIModule:
    """
    AI ëª¨ë“ˆ - ê°•í™”í•™ìŠµ ê¸°ë°˜ ê²Œì„ AI
    
    Chloeê°€ êµ¬í˜„í•  ì£¼ìš” ê¸°ëŠ¥:
    1. PPO/DQN ì •ì±… ë¡œë“œ ë° ì¶”ë¡ 
    2. ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì •
    3. ìê°€ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
    4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, model_path: Optional[str] = None, algorithm: str = "PPO"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ
            algorithm: ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ ("PPO" ë˜ëŠ” "DQN")
        """
        self.model_path = model_path
        self.algorithm = algorithm
        # PyTorchê°€ ì—†ìœ¼ë©´ deviceëŠ” None (ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ)
        if TORCH_AVAILABLE:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = None
        
        # ëª¨ë¸ë“¤
        self.policy_net = None
        self.value_net = None
        self.ppo_model = None
        self.dqn_model = None
        
        # ì„±ëŠ¥ ì¶”ì 
        self.decision_times = []
        self.action_history = []
        self.reward_history = []
        
        # RL ê³„ì¸¡ (Chloeê°€ êµ¬í˜„)
        self.rl_logger = None
        
        # ì´ˆê¸°í™”
        self._initialize_model()
    
    def _initialize_model(self):
        """
        ëª¨ë¸ ì´ˆê¸°í™”
        
        TODO for Chloe: ì‹¤ì œ PPO/DQN ëª¨ë¸ ë¡œë“œ êµ¬í˜„
        """
        if self.model_path:
            # TODO: ì‹¤ì œ êµ¬í˜„
            # if self.algorithm == "PPO":
            #     self.ppo_model = PPO.load(self.model_path)
            # elif self.algorithm == "DQN":
            #     self.dqn_model = DQN.load(self.model_path)
            
            print(f"ğŸ¤– [Chloe TODO] {self.algorithm} ëª¨ë¸ ë¡œë“œ: {self.model_path}")
        else:
            # ê¸°ë³¸ ì •ì±… ë„¤íŠ¸ì›Œí¬ (ì‹œë®¬ë ˆì´ì…˜ìš©) - PyTorchê°€ ìˆì„ ë•Œë§Œ
            if TORCH_AVAILABLE:
                self.policy_net = PolicyNetwork().to(self.device)
            print("âš ï¸ ëª¨ë¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        
        # RL ê³„ì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        # TODO: self.rl_logger = RLInstrumentationLogger("web_game_ai")
    
    def make_decision(self, game_state: Dict[str, Any]) -> AIDecisionResult:
        """
        ê²Œì„ ìƒíƒœë¥¼ ë³´ê³  í–‰ë™ ê²°ì •
        
        Args:
            game_state: ê²Œì„ ì—”ì§„ì—ì„œ ë°›ì€ ìƒíƒœ ì •ë³´
            
        Returns:
            AI ì˜ì‚¬ê²°ì • ê²°ê³¼
            
        TODO for Chloe: ì‹¤ì œ PPO/DQN ì¶”ë¡  êµ¬í˜„
        """
        start_time = time.perf_counter()
        
        if self.ppo_model or self.dqn_model:
            # ì‹¤ì œ RL ëª¨ë¸ ì¶”ë¡ 
            result = self._real_rl_decision(game_state)
        else:
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
            result = self._simulate_decision(game_state)
        
        # ì„±ëŠ¥ ì¸¡ì •
        decision_time = time.perf_counter() - start_time
        self.decision_times.append(decision_time)
        self.action_history.append(result.action)
        
        return result
    
    def _simulate_decision(self, game_state: Dict[str, Any]) -> AIDecisionResult:
        """
        ì‹œë®¬ë ˆì´ì…˜ëœ AI ì˜ì‚¬ê²°ì • (í˜„ì¬ êµ¬í˜„)
        
        Chloeê°€ _real_rl_decision()ìœ¼ë¡œ êµì²´í•  ì˜ˆì •
        """
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì˜ì‚¬ê²°ì •
        player_y = game_state.get('player_y', 0.5)
        obstacle_y = game_state.get('obstacle_y', 0.0)
        obstacle_distance = game_state.get('obstacle_distance', 1.0)
        time_to_collision = game_state.get('time_to_collision', 10.0)
        
        # ì˜ì‚¬ê²°ì • ë¡œì§
        if time_to_collision < 1.0 and obstacle_distance < 0.3:
            if player_y > 0.7:  # í”Œë ˆì´ì–´ê°€ ì•„ë˜ìª½ì— ìˆìœ¼ë©´
                action = "jump"
                reasoning = "ì¥ì• ë¬¼ì´ ê°€ê¹Œì›Œì„œ ì í”„"
                confidence = 0.8
            else:
                action = "stay"
                reasoning = "ì´ë¯¸ ìœ„ìª½ì— ìˆì–´ì„œ ëŒ€ê¸°"
                confidence = 0.6
        else:
            # ëœë¤ í–‰ë™ (íƒí—˜)
            actions = ["stay", "jump", "left", "right"]
            weights = [0.4, 0.3, 0.15, 0.15]
            action = np.random.choice(actions, p=weights)
            reasoning = f"íƒí—˜ì  í–‰ë™: {action}"
            confidence = 0.5
        
        # í–‰ë™ í™•ë¥  ë¶„í¬ (ì‹œë®¬ë ˆì´ì…˜)
        action_probs = {
            "stay": 0.4,
            "jump": 0.3,
            "left": 0.15,
            "right": 0.15
        }
        action_probs[action] += 0.2  # ì„ íƒëœ í–‰ë™ì˜ í™•ë¥  ì¦ê°€
        
        return AIDecisionResult(
            action=action,
            confidence=confidence,
            reasoning=reasoning,
            action_probs=action_probs
        )
    
    def _real_rl_decision(self, game_state: Dict[str, Any]) -> AIDecisionResult:
        """
        ì‹¤ì œ ê°•í™”í•™ìŠµ ëª¨ë¸ ì˜ì‚¬ê²°ì •
        
        TODO for Chloe: ì´ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!
        
        êµ¬í˜„ ê°€ì´ë“œ:
        1. ê²Œì„ ìƒíƒœë¥¼ RL ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        2. PPO ë˜ëŠ” DQN ì¶”ë¡  ì‹¤í–‰
        3. í–‰ë™ í™•ë¥  ë¶„í¬ ê³„ì‚°
        4. ìµœì  í–‰ë™ ì„ íƒ
        5. ì˜ì‚¬ê²°ì • ê·¼ê±° ìƒì„±
        """
        try:
            # ìƒíƒœ ë²¡í„° ìƒì„±
            state_vector = self._create_state_vector(game_state)
            
            if self.algorithm == "PPO" and self.ppo_model:
                # TODO: PPO ì¶”ë¡ 
                # action, _states = self.ppo_model.predict(state_vector, deterministic=False)
                # action_probs = self._get_action_probabilities(state_vector)
                
                # ì„ì‹œ: ì‹œë®¬ë ˆì´ì…˜ í˜¸ì¶œ
                return self._simulate_decision(game_state)
                
            elif self.algorithm == "DQN" and self.dqn_model:
                # TODO: DQN ì¶”ë¡ 
                # action, _states = self.dqn_model.predict(state_vector, deterministic=False)
                # q_values = self._get_q_values(state_vector)
                
                # ì„ì‹œ: ì‹œë®¬ë ˆì´ì…˜ í˜¸ì¶œ
                return self._simulate_decision(game_state)
            
        except Exception as e:
            print(f"âŒ RL ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ í´ë°±
            return self._simulate_decision(game_state)
    
    def _create_state_vector(self, game_state: Dict[str, Any]) -> np.ndarray:
        """
        ê²Œì„ ìƒíƒœë¥¼ RL ëª¨ë¸ ì…ë ¥ ë²¡í„°ë¡œ ë³€í™˜
        
        TODO for Chloe: ìƒíƒœ í‘œí˜„ ìµœì í™”
        """
        # 8ì°¨ì› ìƒíƒœ ë²¡í„° ìƒì„±
        state_vector = np.array([
            game_state.get('player_x', 0.5),
            game_state.get('player_y', 0.5),
            game_state.get('player_vy', 0.0),
            game_state.get('on_ground', 0.0),
            game_state.get('obstacle_x', 0.0),
            game_state.get('obstacle_y', 0.0),
            game_state.get('obstacle_distance', 1.0),
            game_state.get('time_to_collision', 10.0)
        ], dtype=np.float32)
        
        return state_vector
    
    def update_reward(self, reward: float, done: bool = False):
        """
        ë³´ìƒ ì—…ë°ì´íŠ¸ (ìê°€ í•™ìŠµìš©)
        
        TODO for Chloe: ì˜¨ë¼ì¸ í•™ìŠµ êµ¬í˜„
        """
        self.reward_history.append(reward)
        
        if self.rl_logger:
            # TODO: RL ê³„ì¸¡ ì‹œìŠ¤í…œì— ê¸°ë¡
            # self.rl_logger.log_step(reward, done)
            pass
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ í•™ìŠµ (ì„ íƒì )
        if done and len(self.reward_history) > 100:
            self._update_policy()
    
    def _update_policy(self):
        """
        ì •ì±… ì—…ë°ì´íŠ¸ (ì˜¨ë¼ì¸ í•™ìŠµ)
        
        TODO for Chloe: PPO/DQN ì˜¨ë¼ì¸ í•™ìŠµ êµ¬í˜„
        """
        # TODO: ì‹¤ì œ ì •ì±… ì—…ë°ì´íŠ¸ êµ¬í˜„
        # 1. ê²½í—˜ ë²„í¼ì—ì„œ ë°°ì¹˜ ìƒ˜í”Œë§
        # 2. ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        # 3. ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        # 4. ì„±ëŠ¥ ë¡œê¹…
        
        print("ğŸ”„ [Chloe TODO] ì •ì±… ì—…ë°ì´íŠ¸ ì‹¤í–‰")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if not self.decision_times:
            return {}
        
        avg_decision_time = np.mean(self.decision_times)
        avg_reward = np.mean(self.reward_history) if self.reward_history else 0
        
        # í–‰ë™ ë¶„í¬ ê³„ì‚°
        action_counts = {}
        for action in self.action_history:
            action_counts[action] = action_counts.get(action, 0) + 1
        
        return {
            'avg_decision_time_ms': avg_decision_time * 1000,
            'avg_reward': avg_reward,
            'total_decisions': len(self.action_history),
            'action_distribution': action_counts,
            'recent_actions': self.action_history[-10:],  # ìµœê·¼ 10ê°œ í–‰ë™
            'algorithm': self.algorithm
        }
    
    def reset_episode(self):
        """ì—í”¼ì†Œë“œ ì´ˆê¸°í™”"""
        if self.rl_logger:
            # TODO: ì—í”¼ì†Œë“œ ì¢…ë£Œ ë¡œê¹…
            # self.rl_logger.log_episode_end(...)
            pass
        
        # íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (ì„ íƒì )
        if len(self.action_history) > 1000:  # ë©”ëª¨ë¦¬ ê´€ë¦¬
            self.action_history = self.action_history[-500:]
            self.reward_history = self.reward_history[-500:]
    
    def save_model(self, save_path: str):
        """
        ëª¨ë¸ ì €ì¥
        
        TODO for Chloe: í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ êµ¬í˜„
        """
        if self.ppo_model:
            self.ppo_model.save(save_path)
        elif self.dqn_model:
            self.dqn_model.save(save_path)
        else:
            # PyTorch ëª¨ë¸ ì €ì¥
            if TORCH_AVAILABLE and self.policy_net:
                torch.save(self.policy_net.state_dict(), save_path)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")


# Chloeê°€ ì‚¬ìš©í•  í—¬í¼ í•¨ìˆ˜ë“¤
def create_reward_function(game_state: Dict[str, Any], action: str, next_state: Dict[str, Any]) -> float:
    """
    ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„
    
    TODO for Chloe: ê²Œì„ì— ë§ëŠ” ë³´ìƒ í•¨ìˆ˜ êµ¬í˜„
    """
    reward = 0.0
    
    # ìƒì¡´ ë³´ìƒ
    if not next_state.get('game_over', False):
        reward += 1.0
    
    # ì¶©ëŒ í˜ë„í‹°
    if next_state.get('game_over', False):
        reward -= 100.0
    
    # ì ìˆ˜ ì¦ê°€ ë³´ìƒ
    score_diff = next_state.get('score', 0) - game_state.get('score', 0)
    reward += score_diff * 10.0
    
    # ë¶ˆí•„ìš”í•œ í–‰ë™ í˜ë„í‹° (ì„ íƒì )
    if action in ["left", "right"] and game_state.get('obstacle_distance', 1.0) > 0.5:
        reward -= 0.1
    
    return reward


def analyze_failure_mode(game_state: Dict[str, Any], action: str) -> str:
    """
    ì‹¤íŒ¨ ëª¨ë“œ ë¶„ì„
    
    Chloeê°€ ë””ë²„ê¹…ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
    """
    if game_state.get('game_over', False):
        obstacle_distance = game_state.get('obstacle_distance', 1.0)
        time_to_collision = game_state.get('time_to_collision', 10.0)
        
        if obstacle_distance < 0.2 and action == "stay":
            return "íšŒí”¼ ì‹¤íŒ¨: ì¥ì• ë¬¼ì´ ê°€ê¹Œìš´ë° í–‰ë™í•˜ì§€ ì•ŠìŒ"
        elif time_to_collision < 0.5 and action in ["left", "right"]:
            return "ì˜ëª»ëœ íšŒí”¼: ì í”„ ëŒ€ì‹  ì¢Œìš° ì´ë™"
        else:
            return "ì¼ë°˜ì ì¸ ì¶©ëŒ"
    
    return "ì •ìƒ"


# ì‚¬ìš© ì˜ˆì‹œ (Chloeê°€ ì°¸ê³ í•  ì½”ë“œ)
if __name__ == "__main__":
    # AI ëª¨ë“ˆ ì´ˆê¸°í™”
    ai_module = AIModule(
        model_path="path/to/ppo_model.zip",  # Chloeê°€ í›ˆë ¨í•œ ëª¨ë¸
        algorithm="PPO"
    )
    
    # í…ŒìŠ¤íŠ¸ ê²Œì„ ìƒíƒœ
    test_state = {
        'player_x': 0.5,
        'player_y': 0.8,
        'player_vy': 0.0,
        'on_ground': 1.0,
        'obstacle_x': 0.6,
        'obstacle_y': 0.3,
        'obstacle_distance': 0.4,
        'time_to_collision': 2.0
    }
    
    # AI ì˜ì‚¬ê²°ì •
    decision = ai_module.make_decision(test_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ì„ íƒëœ í–‰ë™: {decision.action}")
    print(f"ì‹ ë¢°ë„: {decision.confidence:.2f}")
    print(f"ê·¼ê±°: {decision.reasoning}")
    
    # ì„±ëŠ¥ í†µê³„
    stats = ai_module.get_performance_stats()
    print(f"í‰ê·  ì˜ì‚¬ê²°ì • ì‹œê°„: {stats.get('avg_decision_time_ms', 0):.1f}ms")


# ============================================================================
# ë‚œì´ë„ ë ˆë²¨ ì‹œìŠ¤í…œ
# ============================================================================

class AIStrategy:
    """AI ì „ëµ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, level: int, name: str):
        self.level = level
        self.name = name
    
    def make_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """ì˜ì‚¬ê²°ì • ë©”ì„œë“œ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError


class Level1Strategy(AIStrategy):
    """
    Level 1 (Easy) - ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
    
    ì „ëµ:
    - ê¸°ë³¸ì ì¸ ë©”í…Œì˜¤ íšŒí”¼ë§Œ
    - ë³„ì€ ë¬´ì‹œ
    - ì¤‘ì•™ ìœ ì§€ ì „ëµ ì•½í•¨
    """
    
    def __init__(self):
        super().__init__(level=1, name="Easy")
        self.DETECTION_RANGE = 200  # ë©”í…Œì˜¤ ê°ì§€ ë²”ìœ„
        self.DANGER_RANGE = 100     # ìœ„í—˜ íŒì • ë²”ìœ„
    
    def make_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """ê°„ë‹¨í•œ íšŒí”¼ ë¡œì§"""
        player = game_state.get('player', {})
        obstacles = game_state.get('obstacles', [])
        
        player_x = player.get('x', 480)
        player_y = player.get('y', 360)
        player_size = player.get('size', 50)
        player_center_x = player_x + player_size / 2
        
        # ê°€ì¥ ê°€ê¹Œìš´ ë©”í…Œì˜¤ ì°¾ê¸°
        nearest_meteor = None
        nearest_dist = float('inf')
        
        for obs in obstacles:
            if obs.get('type') != 'meteor':
                continue
            
            obs_x = obs.get('x', 0)
            obs_y = obs.get('y', 0)
            obs_size = obs.get('size', 50)
            obs_center_x = obs_x + obs_size / 2
            
            # í”Œë ˆì´ì–´ë³´ë‹¤ ìœ„ìª½ì— ìˆê³ , Xì¶• ë²”ìœ„ ë‚´
            if obs_y < player_y:
                x_overlap = abs(player_center_x - obs_center_x) < self.DETECTION_RANGE
                if x_overlap:
                    dist = abs(player_center_x - obs_center_x) + (player_y - obs_y) * 0.5
                    if dist < nearest_dist:
                        nearest_dist = dist
                        nearest_meteor = obs
        
        # ë©”í…Œì˜¤ íšŒí”¼
        if nearest_meteor and nearest_dist < self.DANGER_RANGE:
            meteor_center_x = nearest_meteor['x'] + nearest_meteor.get('size', 50) / 2
            
            # ë©”í…Œì˜¤ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™
            if meteor_center_x < player_center_x:
                return 'right'
            else:
                return 'left'
        
        return None  # í–‰ë™ ì—†ìŒ


class Level2Strategy(AIStrategy):
    """
    Level 2 (Medium) - ê³ ê¸‰ íœ´ë¦¬ìŠ¤í‹±
    
    ì „ëµ:
    - ë©”í…Œì˜¤ íšŒí”¼ (í–¥ìƒëœ ë¡œì§)
    - ë³„ ìˆ˜ì§‘ ì „ëµ
    - ì¤‘ì•™ ìœ ì§€
    - ìš©ì•” íšŒí”¼
    """
    
    def __init__(self):
        super().__init__(level=2, name="Medium")
        self.METEOR_DETECT_RANGE = 250
        self.METEOR_DANGER_RANGE = 150
        self.STAR_COLLECT_RANGE = 200
        self.EMERGENCY_RANGE = 80
    
    def make_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """í–¥ìƒëœ ì˜ì‚¬ê²°ì • ë¡œì§"""
        player = game_state.get('player', {})
        obstacles = game_state.get('obstacles', [])
        lava = game_state.get('lava', {})
        
        player_x = player.get('x', 480)
        player_y = player.get('y', 360)
        player_size = player.get('size', 50)
        player_center_x = player_x + player_size / 2
        
        WIDTH = 960
        HEIGHT = 720
        
        # ê°€ì¥ ê°€ê¹Œìš´ ë©”í…Œì˜¤ & ë³„ ì°¾ê¸°
        nearest_meteor = None
        nearest_meteor_dist = float('inf')
        nearest_star = None
        nearest_star_dist = float('inf')
        
        for obs in obstacles:
            obj_type = obs.get('type', 'meteor')
            obs_x = obs.get('x', 0)
            obs_y = obs.get('y', 0)
            obs_size = obs.get('size', 50)
            obs_center_x = obs_x + obs_size / 2
            
            # Xì¶• ì¤‘ì²© ì²´í¬
            x_overlap = abs(player_center_x - obs_center_x) < (player_size + obs_size) / 2 + 50
            
            if obj_type == 'meteor':
                if obs_y < player_y and x_overlap:
                    dist = abs(player_center_x - obs_center_x) + (player_y - obs_y) * 0.5
                    if dist < nearest_meteor_dist:
                        nearest_meteor_dist = dist
                        nearest_meteor = obs
            
            elif obj_type == 'star':
                if obs_y < player_y + 200:
                    dist = abs(player_center_x - obs_center_x) + abs(player_y - obs_y) * 0.3
                    if dist < nearest_star_dist:
                        nearest_star_dist = dist
                        nearest_star = obs
        
        # ìš°ì„ ìˆœìœ„ 1: ê¸´ê¸‰ ë©”í…Œì˜¤ íšŒí”¼
        if nearest_meteor and nearest_meteor_dist < self.EMERGENCY_RANGE:
            # ì í”„ë¡œ íšŒí”¼ ì‹œë„
            if player_y >= HEIGHT - player_size - 10:
                return 'jump'
        
        # ìš°ì„ ìˆœìœ„ 2: ë©”í…Œì˜¤ íšŒí”¼
        if nearest_meteor and nearest_meteor_dist < self.METEOR_DANGER_RANGE:
            meteor_center_x = nearest_meteor['x'] + nearest_meteor.get('size', 50) / 2
            
            if meteor_center_x < player_center_x:
                if player_x + player_size < WIDTH - 20:
                    return 'right'
            else:
                if player_x > 20:
                    return 'left'
        
        # ìš°ì„ ìˆœìœ„ 3: ë³„ ìˆ˜ì§‘
        if nearest_star and nearest_star_dist < self.STAR_COLLECT_RANGE:
            star_center_x = nearest_star['x'] + nearest_star.get('size', 30) / 2
            
            # ë³„ ìª½ìœ¼ë¡œ ì´ë™
            if star_center_x < player_center_x - 15:
                if player_x > 10:
                    return 'left'
            elif star_center_x > player_center_x + 15:
                if player_x + player_size < WIDTH - 10:
                    return 'right'
            
            # ë³„ì´ ìœ„ìª½ì— ìˆìœ¼ë©´ ì í”„
            if nearest_star['y'] < player_y - 50 and player_y >= HEIGHT - player_size - 10:
                return 'jump'
        
        # ìš°ì„ ìˆœìœ„ 4: ìš©ì•” íšŒí”¼
        if lava.get('state') in ['warning', 'active']:
            lava_zone_x = lava.get('zone_x', 0)
            lava_zone_width = lava.get('zone_width', 320)
            lava_zone_end = lava_zone_x + lava_zone_width
            
            # í”Œë ˆì´ì–´ê°€ ìš©ì•” ì˜ì—­ ì•ˆì— ìˆìœ¼ë©´
            if player_x + player_size > lava_zone_x and player_x < lava_zone_end:
                # ê°€ì¥ ê°€ê¹Œìš´ ì•ˆì „ êµ¬ì—­ìœ¼ë¡œ ì´ë™
                if player_center_x < WIDTH / 2:
                    if player_x > 20:
                        return 'left'
                else:
                    if player_x + player_size < WIDTH - 20:
                        return 'right'
        
        # ìš°ì„ ìˆœìœ„ 5: ì¤‘ì•™ ìœ ì§€
        center_x = WIDTH / 2
        if player_center_x < center_x - 100:
            if player_x + player_size < WIDTH - 20:
                return 'right'
        elif player_center_x > center_x + 100:
            if player_x > 20:
                return 'left'
        
        return None


class Level3Strategy(AIStrategy):
    """
    Level 3 (Hard) - PPO ëª¨ë¸ ê¸°ë°˜
    
    ì „ëµ:
    - í•™ìŠµëœ PPO ëª¨ë¸ ì‚¬ìš© (models/rl/ppo_agent.pt)
    - ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìµœê³ ê¸‰ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±
    """
    
    def __init__(self, model_path: Optional[str] = None):
        super().__init__(level=3, name="Hard (PPO)")
        self.model_path = model_path
        self.ppo_model = None
        self.fallback_strategy = Level2Strategy()  # í´ë°±ìš© ì „ëµ
        
        # PPO ëª¨ë¸ ë¡œë“œ ì‹œë„
        self._load_ppo_model()
    
    def _load_ppo_model(self):
        """PPO ëª¨ë¸ ë¡œë“œ"""
        if not self.model_path:
            print("âš ï¸ Level 3: PPO ëª¨ë¸ ê²½ë¡œ ì—†ìŒ, íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±")
            return
        
        try:
            model_file = Path(self.model_path)
            if not model_file.exists():
                print(f"âš ï¸ Level 3: PPO ëª¨ë¸ íŒŒì¼ ì—†ìŒ ({self.model_path}), íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±")
                return
            
            # PyTorch ëª¨ë¸ ë¡œë“œ ì‹œë„
            if TORCH_AVAILABLE:
                import torch
                self.ppo_model = torch.load(self.model_path, map_location='cpu')
                self.ppo_model.eval()
                print(f"âœ… Level 3: PPO ëª¨ë¸ ë¡œë“œ ì„±ê³µ ({self.model_path})")
            else:
                print("âš ï¸ Level 3: PyTorch ì—†ìŒ, íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±")
        
        except Exception as e:
            print(f"âš ï¸ Level 3: PPO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({e}), íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±")
            self.ppo_model = None
    
    def make_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """PPO ëª¨ë¸ ë˜ëŠ” í´ë°± ì „ëµ"""
        # PPO ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
        if self.ppo_model is not None:
            try:
                return self._ppo_decision(game_state)
            except Exception as e:
                print(f"âš ï¸ Level 3: PPO ì¶”ë¡  ì˜¤ë¥˜ ({e}), íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±")
        
        # í´ë°±: Level 2 ì „ëµ ì‚¬ìš©
        return self.fallback_strategy.make_decision(game_state)
    
    def _ppo_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """
        PPO ëª¨ë¸ ê¸°ë°˜ ì˜ì‚¬ê²°ì •
        
        TODO for Chloe: ì‹¤ì œ PPO ì¶”ë¡  êµ¬í˜„
        """
        # TODO: ê²Œì„ ìƒíƒœë¥¼ PPO ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        # state_vector = self._encode_state(game_state)
        
        # TODO: PPO ì¶”ë¡ 
        # with torch.no_grad():
        #     action_probs = self.ppo_model(state_vector)
        #     action_idx = torch.argmax(action_probs).item()
        
        # TODO: í–‰ë™ ë§¤í•‘
        # actions = [None, 'jump', 'left', 'right']
        # return actions[action_idx]
        
        # ì„ì‹œ: í´ë°± ì‚¬ìš©
        return self.fallback_strategy.make_decision(game_state)


class Level4Strategy(AIStrategy):
    """
    Level 4 (Expert) - Ensemble ëª¨ë¸
    
    ì „ëµ:
    - PPO + Vision ê¸°ë°˜ ì•™ìƒë¸”
    - ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì •ì„ ê²°í•©
    - ê°€ì¥ ë†’ì€ ì„±ëŠ¥ ëª©í‘œ
    """
    
    def __init__(self, ppo_model_path: Optional[str] = None, dqn_model_path: Optional[str] = None):
        super().__init__(level=4, name="Expert (Ensemble)")
        self.ppo_strategy = Level3Strategy(model_path=ppo_model_path)
        self.base_strategy = Level2Strategy()
        self.dqn_model_path = dqn_model_path
        self.dqn_model = None
        
        # DQN ëª¨ë¸ ë¡œë“œ ì‹œë„ (ì„ íƒì )
        self._load_dqn_model()
    
    def _load_dqn_model(self):
        """DQN ëª¨ë¸ ë¡œë“œ (ì„ íƒì )"""
        if not self.dqn_model_path:
            return
        
        try:
            model_file = Path(self.dqn_model_path)
            if not model_file.exists():
                print(f"âš ï¸ Level 4: DQN ëª¨ë¸ íŒŒì¼ ì—†ìŒ ({self.dqn_model_path})")
                return
            
            if TORCH_AVAILABLE:
                import torch
                self.dqn_model = torch.load(self.dqn_model_path, map_location='cpu')
                self.dqn_model.eval()
                print(f"âœ… Level 4: DQN ëª¨ë¸ ë¡œë“œ ì„±ê³µ ({self.dqn_model_path})")
        
        except Exception as e:
            print(f"âš ï¸ Level 4: DQN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({e})")
            self.dqn_model = None
    
    def make_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """ì•™ìƒë¸” ì˜ì‚¬ê²°ì •"""
        # ì—¬ëŸ¬ ì „ëµì˜ ê²°ì •ì„ ìˆ˜ì§‘
        decisions = []
        
        # PPO ì „ëµ
        ppo_action = self.ppo_strategy.make_decision(game_state)
        if ppo_action:
            decisions.append(('ppo', ppo_action, 0.5))  # ê°€ì¤‘ì¹˜ 0.5
        
        # íœ´ë¦¬ìŠ¤í‹± ì „ëµ
        heuristic_action = self.base_strategy.make_decision(game_state)
        if heuristic_action:
            decisions.append(('heuristic', heuristic_action, 0.3))  # ê°€ì¤‘ì¹˜ 0.3
        
        # DQN ì „ëµ (ìˆìœ¼ë©´)
        if self.dqn_model is not None:
            # TODO: DQN ì¶”ë¡  êµ¬í˜„
            # dqn_action = self._dqn_decision(game_state)
            # decisions.append(('dqn', dqn_action, 0.2))
            pass
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ íˆ¬í‘œ
        if not decisions:
            return None
        
        # ê°„ë‹¨í•œ ì•™ìƒë¸”: ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ì˜ í–‰ë™ ì„ íƒ
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì•™ìƒë¸” ë°©ë²• ì‚¬ìš© ê°€ëŠ¥
        decisions.sort(key=lambda x: x[2], reverse=True)
        return decisions[0][1]


class AILevelManager:
    """AI ë‚œì´ë„ ë ˆë²¨ ê´€ë¦¬ì"""
    
    def __init__(self, ppo_model_path: Optional[str] = None, dqn_model_path: Optional[str] = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            ppo_model_path: Level 3, 4ì—ì„œ ì‚¬ìš©í•  PPO ëª¨ë¸ ê²½ë¡œ
            dqn_model_path: Level 4ì—ì„œ ì‚¬ìš©í•  DQN ëª¨ë¸ ê²½ë¡œ (ì„ íƒì )
        """
        self.strategies = {
            1: Level1Strategy(),
            2: Level2Strategy(),
            3: Level3Strategy(model_path=ppo_model_path),
            4: Level4Strategy(ppo_model_path=ppo_model_path, dqn_model_path=dqn_model_path)
        }
        self.current_level = 1
    
    def set_level(self, level: int):
        """ë‚œì´ë„ ë ˆë²¨ ì„¤ì •"""
        if level not in self.strategies:
            raise ValueError(f"Invalid level: {level}. Must be 1, 2, 3, or 4.")
        self.current_level = level
        print(f"ğŸ® AI ë‚œì´ë„: Level {level} ({self.strategies[level].name})")
    
    def make_decision(self, game_state: Dict[str, Any]) -> Optional[str]:
        """í˜„ì¬ ë ˆë²¨ì˜ ì „ëµìœ¼ë¡œ ì˜ì‚¬ê²°ì •"""
        strategy = self.strategies[self.current_level]
        return strategy.make_decision(game_state)
    
    def get_level_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ë ˆë²¨ ì •ë³´ ë°˜í™˜"""
        strategy = self.strategies[self.current_level]
        return {
            'level': self.current_level,
            'name': strategy.name,
            'description': f"Level {self.current_level}: {strategy.name}"
        }
